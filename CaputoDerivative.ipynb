{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd9e32c3-31b3-418c-8fac-f336a7e09afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "import numpy as np\n",
    "from pikan.model_utils import GeneralizedMLP, sobol_sample\n",
    "from jax import grad, vmap, jit\n",
    "from jax.scipy.special import gamma\n",
    "from functools import partial\n",
    "import optax\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c36ab0e4-70b3-4d10-b913-5a7e66c6d61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.8603496], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GeneralizedMLP(\n",
    "    kernel_init=nn.initializers.glorot_normal(),\n",
    "    num_input=2,\n",
    "    num_output=1,\n",
    "    use_fourier_feats=True,\n",
    "    layer_sizes=[128, 128],\n",
    ")\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "collocs = jnp.ones((2))\n",
    "params = model.init(key, collocs)['params']\n",
    "model.apply({\"params\": params}, collocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ddc20b-78bf-4042-9a82-e7f600791831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.10571431], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the inference function\n",
    "def inference(params, model, x, t):\n",
    "    x = jnp.stack([x,t])\n",
    "    return model.apply({'params': params}, x)\n",
    "\n",
    "inference(params, model, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f351fba-b150-4178-80d5-99abb847eb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caputo Derivative: 1.4690284\n"
     ]
    }
   ],
   "source": [
    "def inference(params, x, t):\n",
    "    output = model.apply({'params': params}, jnp.array([x, t]))\n",
    "    return output[0]\n",
    "\n",
    "def get_caputo_derivative(inference):\n",
    "    @jax.jit\n",
    "    def caputo_derivative(params, x, t, alpha, dt=1e-3, num_steps = 10000):\n",
    "        \"\"\"\n",
    "        Compute the Caputo derivative of order alpha for a function f(x, t) with respect to t.\n",
    "    \n",
    "        Parameters:\n",
    "        - f: A function f(x, t) that takes two arguments, x and t.\n",
    "        - x: The spatial variable.\n",
    "        - t: The time variable.\n",
    "        - alpha: The order of the Caputo derivative (0 < alpha < 1).\n",
    "        - dt: The time step for discretization.\n",
    "    \n",
    "        Returns:\n",
    "        - The Caputo derivative of f(x, t) at time t.\n",
    "        \"\"\"\n",
    "        # Define the integrand\n",
    "        def integrand(tau):\n",
    "            return grad(inference, 2)(params, x, tau) / (t - tau)**alpha\n",
    "    \n",
    "        # Fixed number of steps for static shape\n",
    "        tau_values = jnp.linspace(0, t - dt, num_steps)  # Exclude t\n",
    "        integrand_values = vmap(integrand)(tau_values)\n",
    "    \n",
    "        # Compute the integral using the trapezoidal rule\n",
    "        integral = jnp.trapezoid(integrand_values, tau_values)\n",
    "    \n",
    "        # Normalize by the gamma function\n",
    "        return integral / gamma(1 - alpha)\n",
    "    \n",
    "    return caputo_derivative\n",
    "    \n",
    "x = 1.0\n",
    "t = 1.0\n",
    "alpha = 0.5\n",
    "\n",
    "caputo_derivative_fn = get_caputo_derivative(inference)\n",
    "caputo_deriv = caputo_derivative_fn(params, x, t, alpha)\n",
    "\n",
    "print(\"Caputo Derivative:\", caputo_deriv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47b3954f-c1fd-4ddf-b27e-967dd3ffeaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([ 0.352297  ,  0.07153108,  0.9043222 , -1.1922547 ,  1.1729538 ,\n",
       "         0.7631223 ,  2.1819003 ,  0.29982382,  1.008047  ,  2.9932196 ,\n",
       "        -1.1980772 , -1.2293242 , -0.48812997,  0.40089384,  1.9867368 ,\n",
       "        -0.26317292, -0.30621415,  1.7191843 ,  0.6605068 , -1.1688274 ,\n",
       "        -1.2866187 , -0.56121   ,  2.1097345 , -2.1567328 ,  1.998726  ,\n",
       "         2.600072  ,  1.7475344 ,  0.35622153,  0.20362371, -1.2614639 ,\n",
       "        -0.8023467 ,  1.7286482 ,  0.80583006,  1.7793441 ,  0.0047167 ,\n",
       "        -1.3672754 , -0.34065434,  0.40130576,  2.1432748 , -2.4950643 ,\n",
       "         1.7482466 ,  1.1350166 ,  1.0107727 ,  1.9275477 , -1.0320338 ,\n",
       "        -1.9398096 ,  0.8786229 ,  1.0118887 ,  1.9089093 ,  3.3738277 ,\n",
       "        -2.0842304 , -0.6886029 ,  1.3699871 , -0.94748884, -0.77802664,\n",
       "         1.131459  , -0.40089476,  2.266505  , -0.867456  , -1.2834089 ,\n",
       "         0.36519396,  0.6913541 ,  1.5833902 ,  0.01851468], dtype=float32),\n",
       " (64, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BS = 64\n",
    "collocs = sobol_sample(np.array([-1, 0]), np.array([1, 1]), BS)\n",
    "\n",
    "jax.vmap(caputo_derivative_fn, (None, 0,0,None))(params, collocs[:, 0], collocs[:, 1], alpha), collocs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fa24ce9-2b42-43c0-bba9-77fda69ffbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(70966.03, dtype=float32), Array(0, dtype=int32, weak_type=True))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# caputo diffusion on 1d\n",
    "class fractional_diffusion():\n",
    "    def __init__(self, model, bc_l, bc_r, ic_func, alpha, dom=[-1,1]):\n",
    "        self.bc_l = bc_l # boundary vals\n",
    "        self.bc_r = bc_r\n",
    "\n",
    "        self.ic_collocs = jnp.linspace(dom[0], dom[1], 1000)\n",
    "        self.ic_vals = ic_func(self.ic_collocs) # t_0 values\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.dom = dom\n",
    "\n",
    "        self.caputo_derivative = get_caputo_derivative(self.neural_net)\n",
    "\n",
    "        # paralellize for faster computation\n",
    "        self.neural_net_fn = jax.vmap(self.neural_net, (None, 0, 0))\n",
    "        self.residual_loss_fn = jax.vmap(self.residual_loss, (None, 0, 0))\n",
    "\n",
    "    def neural_net(self, params, x, t):\n",
    "        output = model.apply({'params': params}, jnp.array([x, t]))\n",
    "        return output[0]\n",
    "\n",
    "    def residual_loss(self, params, x, t):\n",
    "        f_derivative = self.caputo_derivative(params, x, t, self.alpha)\n",
    "        laplacian = grad(grad(self.neural_net, argnums=1), argnums=1)(params, x, t)\n",
    "        \n",
    "        return f_derivative - laplacian\n",
    "\n",
    "    def mse(self, arr):\n",
    "        return jnp.sum(arr**2)\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss(self, params, collocs):\n",
    "        ic_loss = self.neural_net_fn(params, self.ic_collocs, jnp.zeros_like(self.ic_collocs))\n",
    "        bc_l_loss = self.neural_net_fn(params, jnp.full_like(collocs[:,1], self.dom[0]), collocs[:,1])\n",
    "        bc_r_loss = self.neural_net_fn(params, jnp.full_like(collocs[:,1], self.dom[1]), collocs[:,1])\n",
    "\n",
    "        eq_loss = self.residual_loss_fn(params, collocs[:,0], collocs[:,1])\n",
    "\n",
    "        # losses could be added with custom weights, TODO gradnorm\n",
    "        loss = self.mse(ic_loss) + self.mse(bc_l_loss) + self.mse(bc_r_loss) + self.mse(eq_loss)\n",
    "\n",
    "        new_loc_w = 0 # maybe implement local weights\n",
    "        return loss, new_loc_w \n",
    "\n",
    "fdiff = fractional_diffusion(model, 0, 0, ic_func=lambda x: jnp.cos(x*jnp.pi/2), alpha=.5)\n",
    "fdiff.neural_net(params, 0., 1.), fdiff.residual_loss(params, 0., 1.)\n",
    "\n",
    "BS = 64\n",
    "collocs = sobol_sample(np.array([-1, 0]), np.array([1, 1]), BS)\n",
    "fdiff.loss(params, collocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a91c0d76-0595-48d2-81bf-2b074675c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your gradient function\n",
    "grad_fn = jax.value_and_grad(fdiff.loss, has_aux=True)\n",
    "\n",
    "# Define the training loop\n",
    "def train_step(params, collocs, opt_state):\n",
    "    # Compute loss and gradients\n",
    "    (loss, new_loc_w), grads = grad_fn(params, collocs)\n",
    "\n",
    "    # Apply gradients to update the parameters\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    return params, opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b90462d9-69bb-424d-88db-b876e7c0fdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "\n",
    "# Define a cosine decay learning rate schedule\n",
    "# Learning rate schedule (cosine decay)\n",
    "schedule_fn = optax.cosine_decay_schedule(\n",
    "    init_value=1e-3,      # Initial learning rate\n",
    "    decay_steps=2000,     # Total number of decay steps\n",
    "    alpha=0.1             # Final learning rate multiplier\n",
    ")\n",
    "\n",
    "# Optimizer setup with Adam\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=schedule_fn,\n",
    "    b1=0.9,               # Beta1\n",
    "    b2=0.999,             # Beta2\n",
    "    eps=1e-8              # Epsilon\n",
    ")\n",
    "\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "355c9c28-3437-45ca-8f73-0f7e3d57aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save parameters and state\n",
    "def save_checkpoint(params, opt_state, epoch, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump({'params': params, 'opt_state': opt_state, 'epoch': epoch}, f)\n",
    "    print(f\"Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "def load_checkpoint(filename, params, state):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            checkpoint = pickle.load(f)\n",
    "        print(f\"Checkpoint loaded from epoch {checkpoint['epoch']}\")\n",
    "        return checkpoint['params'], checkpoint['opt_state'], checkpoint['epoch']\n",
    "    return params, state, 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa6e57-36ac-4b97-9995-b7b07e141e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss  30014.42382812:   0%|                  | 1/1000 [00:10<2:59:52, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss  30014.42382812:   1%|▏                 | 9/1000 [00:58<1:40:07,  6.06s/it]"
     ]
    }
   ],
   "source": [
    "# Define constants\n",
    "BS = 64\n",
    "EPOCHS = 1000\n",
    "TMAX = 1\n",
    "CHECKPOINT_FILE = \"diff_v1.pkl\"\n",
    "\n",
    "# Initialize or load checkpoint\n",
    "params, opt_state, start_epoch = load_checkpoint(CHECKPOINT_FILE, params, opt_state)\n",
    "\n",
    "# Main training loop\n",
    "for i in (pbar := tqdm(range(start_epoch, EPOCHS))):\n",
    "    collocs = sobol_sample(np.array([-1, 0]), np.array([1, TMAX]), BS)\n",
    "    params, opt_state, loss = train_step(params, collocs, opt_state)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        pbar.set_description(f\"Loss {loss: .8f}\")\n",
    "    \n",
    "    if i % 50 == 0:  # Save every x epochs\n",
    "        save_checkpoint(params, opt_state, i, CHECKPOINT_FILE)\n",
    "\n",
    "save_checkpoint(params, opt_state, i, CHECKPOINT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb6f49a-0566-43ad-963a-2554e11ce40a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
