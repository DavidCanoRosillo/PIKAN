{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09255fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pikan.model_utils import GeneralizedMLP, PirateNet\n",
    "from pikan.model_utils import get_mse_loss, get_train_step\n",
    "from pikan.model_utils import KeyHandler, sobol_sample\n",
    "from pikan.utils import load_dict_from_file, save_dict_to_file\n",
    "\n",
    "from pikan.interpolated_funcs import circular_wave_interference\n",
    "\n",
    "import yaml\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import jax\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c154d0-3d12-4bcb-b439-34f2e989a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jaxkan==0.1.7# --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca25d777-1196-4a22-af02-6dd8a8c91acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34169b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_error(results, true):\n",
    "    err = jnp.sum((results - true)**2) / jnp.sum(true**2)\n",
    "    err = jnp.sqrt(err)\n",
    "    return err\n",
    "\n",
    "def get_l2_error(config, variables):\n",
    "    model = get_model(config)\n",
    "    learnable_func = get_target_func(experiment)\n",
    "    \n",
    "    N = 300\n",
    "    X_1 = jnp.linspace(experiment[\"X_MIN\"], experiment[\"X_MAX\"], N)\n",
    "    X_2 = jnp.linspace(experiment[\"Y_MIN\"], experiment[\"Y_MAX\"], N)\n",
    "    X_1, X_2 = jnp.meshgrid(X_1, X_2, indexing='ij')\n",
    "    coords = jnp.stack([X_1.flatten(), X_2.flatten()], axis=1)\n",
    "\n",
    "    y = learnable_func(coords).reshape(-1,1)\n",
    "\n",
    "    if config[\"MODEL\"] == \"KAN\":\n",
    "        y_hat = model.apply(variables, coords)\n",
    "    else:\n",
    "        y_hat = model.apply({\"params\": variables[\"params\"]}, coords)\n",
    "        \n",
    "    err = l2_error(y_hat, y)\n",
    "    \n",
    "    return err\n",
    "\n",
    "def sum_params(data, verbose=False):\n",
    "    total = 0\n",
    "    if isinstance(data, type(jnp.array([]))):  # If the current node is a leaf array\n",
    "        return len(data.reshape(-1))\n",
    "    elif isinstance(data, dict):  # If the current node is a dictionary\n",
    "        for key, value in data.items():\n",
    "            if verbose:\n",
    "                print(f\"Processing key: {key}\")  # Print the current key\n",
    "            branch_total = sum_params(value)  # Compute the total for this subbranch\n",
    "            if verbose:\n",
    "                print(f\"Total parameters in subbranch '{key}': {branch_total}\")\n",
    "            total += branch_total\n",
    "    \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c200a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config):\n",
    "    if config[\"MODEL\"] == \"MLP\":\n",
    "        return GeneralizedMLP(\n",
    "            kernel_init=nn.initializers.glorot_normal(),\n",
    "            num_input=config['N_INPUT'],\n",
    "            num_output=1,\n",
    "            use_fourier_feats=config['FourierFeatures'],\n",
    "            layer_sizes=config['layers']\n",
    "        )\n",
    "    if config[\"MODEL\"] == \"KAN\":\n",
    "        from pikan.model_utils import FourierKAN\n",
    "        \n",
    "        return FourierKAN(\n",
    "            kernel_init=nn.initializers.glorot_normal(),\n",
    "            num_input=config['N_INPUT'],\n",
    "            num_output=1,\n",
    "            use_fourier_feats=config['FourierFeatures'],\n",
    "            layer_sizes=config['layers']\n",
    "        )\n",
    "    if config[\"MODEL\"] == \"PIRATE\":\n",
    "        return PirateNet(\n",
    "            kernel_init=nn.initializers.glorot_normal(),\n",
    "            num_input=config['N_INPUT'],\n",
    "            num_output=1,\n",
    "            layer_sizes=config['layers'] # first is fourier\n",
    "        )\n",
    "    \n",
    "def get_target_func(config):\n",
    "    if config[\"learnable_func\"] == \"circular_wave_interference\":\n",
    "        learnable_func = circular_wave_interference\n",
    "\n",
    "    learnable_func = partial(learnable_func, FREQ=config[\"FREQ\"])\n",
    "    return learnable_func\n",
    "\n",
    "def sample_collocs(config):\n",
    "    collocs = jnp.array(sobol_sample(np.array([config[\"X_MIN\"],config[\"Y_MIN\"]]), \n",
    "                                     np.array([config[\"X_MAX\"],config[\"Y_MAX\"]]), config[\"BS\"]))\n",
    "    return collocs\n",
    "\n",
    "def train_model(config):    \n",
    "    collocs = sample_collocs(config)\n",
    "\n",
    "    model = get_model(config)\n",
    "    variables = model.init(keygen.key(), collocs)\n",
    "    loss_fn = get_mse_loss(model, MODEL=config[\"MODEL\"])\n",
    "    \n",
    "    # Define a cosine decay learning rate schedule\n",
    "    schedule_fn = optax.cosine_decay_schedule(\n",
    "        init_value=1e-2,       # Initial learning rate\n",
    "        decay_steps=config[\"EPOCHS\"]+1,  # Total number of decay steps\n",
    "        alpha=1e-3             # Final learning rate multiplier\n",
    "    )\n",
    "    optimizer = optax.adamw(learning_rate=schedule_fn, weight_decay=1e-4)\n",
    "    opt_state = optimizer.init(variables['params'])\n",
    "    train_step = jax.jit(get_train_step(model, optimizer, loss_fn))\n",
    "\n",
    "    learnable_func = get_target_func(config)\n",
    "\n",
    "    if config[\"MODEL\"] != \"KAN\":\n",
    "        variables[\"state\"] = []\n",
    "\n",
    "    # train always on same colloc points\n",
    "    collocs = sample_collocs(experiment)\n",
    "    losses = []\n",
    "\n",
    "    if experiment[\"EPOCHS\"]:\n",
    "        loc_w = jnp.array([])\n",
    "        for i in (pbar:= tqdm(range(experiment[\"EPOCHS\"]))):\n",
    "            params, state = variables['params'], variables['state']\n",
    "            y = learnable_func(collocs).reshape(-1,1)\n",
    "            params, opt_state, loss, loc_w = train_step(params, collocs, y,\n",
    "                                                        opt_state, state, loc_w)\n",
    "            variables = {'params': params, 'state':state}\n",
    "    \n",
    "            losses.append(loss)\n",
    "    \n",
    "            if i % 50 == 0: # dont waste a lot of time printing\n",
    "                pbar.set_description(f\"Loss {loss: .8f}\")\n",
    "\n",
    "    return variables, collocs, learnable_func, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "833880cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['increase_freq_1', 'increase_freq_2', 'increase_freq_3', 'increase_freq_4', 'increase_freq_5', 'increase_freq_6', 'increase_freq_7', 'increase_freq_8', 'increase_freq_9', 'increase_freq_10'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"increase_freq_KAN_Fourier\"\n",
    "# filename = \"increase_freq_PIRATE\"\n",
    "# filename = \"loss_landscape\"\n",
    "\n",
    "with open(f\"yaml_configs/{filename}.yaml\", 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "keygen = KeyHandler(0)\n",
    "config[\"experiments\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3c17519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increase_freq_1\n",
      "12513\n",
      "increase_freq_2\n",
      "12513\n",
      "increase_freq_3\n",
      "12513\n",
      "increase_freq_4\n",
      "12513\n",
      "increase_freq_5\n",
      "12513\n",
      "increase_freq_6\n",
      "12513\n",
      "increase_freq_7\n",
      "12513\n",
      "increase_freq_8\n",
      "12513\n",
      "increase_freq_9\n",
      "12513\n",
      "increase_freq_10\n",
      "12513\n"
     ]
    }
   ],
   "source": [
    "for exp_key in config[\"experiments\"].keys():\n",
    "    experiment = config[\"experiments\"][exp_key]\n",
    "\n",
    "    collocs = sample_collocs(experiment)\n",
    "    model = get_model(experiment)\n",
    "    variables = model.init(keygen.key(), collocs)\n",
    "\n",
    "    print(exp_key)\n",
    "    print(sum_params(variables[\"params\"], verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ccbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss  0.38238314:   0%|                     | 3/2500 [01:24<21:01:38, 30.32s/it]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "utils = {}\n",
    "elapsed_times = []\n",
    "for exp_key in config[\"experiments\"].keys():\n",
    "    experiment = config[\"experiments\"][exp_key]\n",
    "    start_time = time.time()\n",
    "    variables, collocs, learnable_func, loss_fn = train_model(experiment)\n",
    "    \n",
    "    model = get_model(experiment)\n",
    "    utils[exp_key] = (variables, model, collocs, learnable_func)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    save_dict_to_file(variables, f\"results/models/{filename}\", f\"{exp_key}\")\n",
    "    elapsed_times.append(end_time - start_time)\n",
    "\n",
    "print(\"Elapsed times for each iteration:\", elapsed_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad1981c-d40d-428e-beb6-e94d2235745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Function to plot the loss landscape (2D and 3D)\n",
    "def plot_loss_landscape(grid_x, grid_y, loss_values, title, exp_key, ax=None, plot_3d=False):\n",
    "    # Create meshgrid\n",
    "    X_grid, Y_grid = np.meshgrid(grid_x, grid_y)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(10, 7))\n",
    "        if plot_3d:\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "        else:\n",
    "            ax = fig.add_subplot(111)\n",
    "    \n",
    "    if plot_3d:\n",
    "        # Plot the 3D surface\n",
    "        surf = ax.plot_surface(\n",
    "            X_grid, Y_grid, loss_values, cmap='inferno', antialiased=True, edgecolor='none'\n",
    "        )\n",
    "        ax.set_zlabel('Loss')\n",
    "        # Add labels and title\n",
    "        ax.set_xlabel('Alpha')\n",
    "        ax.set_ylabel('Beta')\n",
    "    else:\n",
    "        # Plot the 2D contour\n",
    "        contour = ax.contourf(X_grid, Y_grid, loss_values, levels=50, cmap='viridis')\n",
    "        plt.colorbar(contour, ax=ax)\n",
    "        \n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Save the figure\n",
    "    if ax is None:\n",
    "        fig.savefig(f'results/visuals/loss_landscapes/{exp_key}')\n",
    "        plt.show()\n",
    "\n",
    "# Main loop\n",
    "loss_landscape = False\n",
    "\n",
    "if loss_landscape:\n",
    "    for key, aux in tqdm(utils.items()):\n",
    "        variables, model, collocs, learnable_func = aux\n",
    "        print(key)\n",
    "        \n",
    "        # Create a figure with subplots for each repetition (2D and 3D)\n",
    "        fig_2d, axes_2d = plt.subplots(1, 3, figsize=(15, 5))  # 1 row, 3 columns for 2D plots\n",
    "        fig_3d = plt.figure(figsize=(15, 5))  # Separate figure for 3D plots\n",
    "        fig_2d.suptitle(f'Loss Landscape for {key} (2D)', fontsize=16)\n",
    "        fig_3d.suptitle(f'Loss Landscape for {key} (3D)', fontsize=16)\n",
    "        \n",
    "        for repetition in range(3):\n",
    "            grid_x, grid_y, loss_vals = compute_loss_landscape(variables[\"params\"], model, \n",
    "                                                              collocs, learnable_func(collocs), res=.5)\n",
    "            \n",
    "            # Plot the 2D loss landscape on the corresponding subplot\n",
    "            plot_loss_landscape(grid_x, grid_y, loss_vals, f'Repetition {repetition + 1}', \n",
    "                               f'{key}_{repetition}_2d', ax=axes_2d[repetition], plot_3d=False)\n",
    "            \n",
    "            # Plot the 3D loss landscape\n",
    "            ax_3d = fig_3d.add_subplot(1, 3, repetition + 1, projection='3d')\n",
    "            plot_loss_landscape(grid_x, grid_y, loss_vals, f'Repetition {repetition + 1}', \n",
    "                               f'{key}_{repetition}_3d', ax=ax_3d, plot_3d=True)\n",
    "        \n",
    "        # Adjust layout to minimize blank space\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save and show figures\n",
    "        fig_2d.savefig(f'results/visuals/loss_landscapes/{key}_2d')\n",
    "        fig_3d.savefig(f'results/visuals/loss_landscapes/{key}_3d')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abdf919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(config[\"experiments\"]).T\n",
    "df[\"params\"] = -1\n",
    "df[\"L2%\"] = -1\n",
    "\n",
    "for idx, exp_key in enumerate(config[\"experiments\"].keys()):\n",
    "    experiment = config[\"experiments\"][exp_key]\n",
    "    \n",
    "    variables = load_dict_from_file(f\"results/models/{filename}/{exp_key}\")\n",
    "    l2_err = get_l2_error(experiment, variables)\n",
    "\n",
    "    df.loc[exp_key, \"params\"] = sum_params(variables[\"params\"], verbose=False)\n",
    "    df.loc[exp_key, \"L2%\"] = l2_err * 100\n",
    "    df['seconds'] = elapsed_times\n",
    "    \n",
    "    print(f\"Results from {exp_key}:\")\n",
    "    print(f\"L2 {l2_err*100:.4f}%\")\n",
    "    print(f\"#params {df.iloc[idx]['params']}\")\n",
    "    \n",
    "df.to_csv(f\"results/csvs/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad3f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from matplotlib import cm\n",
    "import os\n",
    "\n",
    "def plot_true_approx(config, variables, save_folder, exp_key):\n",
    "    model = get_model(config)\n",
    "    learnable_func = get_target_func(config)  # Fixed to use 'config' instead of 'experiment'\n",
    "    \n",
    "    N = 300\n",
    "    X_1 = jnp.linspace(config[\"X_MIN\"], config[\"X_MAX\"], N)  # Fixed to use 'config'\n",
    "    X_2 = jnp.linspace(config[\"Y_MIN\"], config[\"Y_MAX\"], N)  # Fixed to use 'config'\n",
    "    X_1, X_2 = jnp.meshgrid(X_1, X_2, indexing='ij')\n",
    "    coords = jnp.stack([X_1.flatten(), X_2.flatten()], axis=1)\n",
    "\n",
    "    y = learnable_func(coords).reshape(-1, 1)\n",
    "    if config[\"MODEL\"] == \"KAN\":\n",
    "        y_hat = model.apply(variables, coords)\n",
    "    else:\n",
    "        y_hat = model.apply({\"params\": variables[\"params\"]}, coords)\n",
    "\n",
    "    y_hat = y_hat.reshape(-1, 1)\n",
    "\n",
    "    # Compute the absolute error\n",
    "    abs_error = jnp.abs(y - y_hat).reshape(-1, 1)\n",
    "\n",
    "    # Create a figure and axis grid for the 3 subplots\n",
    "    fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "    # Plotting the approximated function (y_hat) on the left using the plasma colormap\n",
    "    ax1 = fig.add_subplot(1, 3, 1, projection='3d')\n",
    "    ax1.plot_trisurf(coords[:, 0], coords[:, 1], y_hat.flatten(), cmap=cm.plasma)\n",
    "    ax1.set_title('Approximated Function (y_hat)')\n",
    "    ax1.set_xlabel('X1')\n",
    "    ax1.set_ylabel('X2')\n",
    "    ax1.set_zlabel('y_hat')\n",
    "\n",
    "    # Plotting the true function (y) in the middle\n",
    "    ax2 = fig.add_subplot(1, 3, 2, projection='3d')\n",
    "    ax2.plot_trisurf(coords[:, 0], coords[:, 1], y.flatten(), cmap='viridis')\n",
    "    ax2.set_title('True Function (y)')\n",
    "    ax2.set_xlabel('X1')\n",
    "    ax2.set_ylabel('X2')\n",
    "    ax2.set_zlabel('y')\n",
    "\n",
    "    # Plotting the absolute error on the right\n",
    "    ax3 = fig.add_subplot(1, 3, 3, projection='3d')\n",
    "    ax3.plot_trisurf(coords[:, 0], coords[:, 1], abs_error.flatten(), cmap='inferno')\n",
    "    ax3.set_title('Absolute Error')\n",
    "    ax3.set_xlabel('X1')\n",
    "    ax3.set_ylabel('X2')\n",
    "    ax3.set_zlabel('Error')\n",
    "\n",
    "    # Adjust layout to ensure all plots fit well within the figure\n",
    "    plt.tight_layout(pad=3.0)\n",
    "\n",
    "    # Saving the figure\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    save_path = os.path.join(save_folder, f'{exp_key}_plot.png')\n",
    "\n",
    "    # Save the plot to the specified path\n",
    "    fig.savefig(save_path)\n",
    "\n",
    "    # Explicitly close the plot to prevent it from showing\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58de681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "\n",
    "def process_experiment(exp_key):\n",
    "    try:\n",
    "        experiment = config[\"experiments\"][exp_key]\n",
    "        print(f\"Results from {exp_key}:\")\n",
    "        variables = load_dict_from_file(f'results/models/{filename}/{exp_key}')\n",
    "        plot_true_approx(experiment, variables, f\"results/visuals/{filename}\", exp_key)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {exp_key}: {e}\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "    # Ensure we exhaust the map generator\n",
    "    list(executor.map(process_experiment, config[\"experiments\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8daeb4-62d3-4542-8ba0-2a61fcc0968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.read_csv('results/csvs/increase_freq_fourier').tail(3))\n",
    "\n",
    "display(pd.read_csv('results/csvs/increase_freq_KAN_Fourier').tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d936bd5-2cb3-4778-934d-6314a2cd03b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
