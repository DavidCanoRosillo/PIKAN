{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae86c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import flax.linen as nn\n",
    "from flax.core import freeze, unfreeze\n",
    "import jax.numpy as jnp\n",
    "from flax.linen.initializers import glorot_normal\n",
    "from typing import Callable\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Utils import *\n",
    "\n",
    "r = KeyHandler(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e1b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import *\n",
    "\n",
    "class Config:\n",
    "    N_INPUT = 2\n",
    "    \n",
    "    X_MIN = -1\n",
    "    X_MAX = 1\n",
    "    \n",
    "    Y_MIN = -1\n",
    "    Y_MAX = 1\n",
    "    \n",
    "    BS = 2**12 #14\n",
    "    BS_BC = 1024\n",
    "    EPOCHS = round(0.25*10000) # 2 * 10000\n",
    "    \n",
    "    MODEL = 'MLP'\n",
    "    layer_dims = [N_INPUT, 7, 7, 1]\n",
    "    \n",
    "    HIDDEN_UNITS = 64\n",
    "    FourierFeatures = True\n",
    "    \n",
    "r = KeyHandler(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "710f1636",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RWF(nn.Module):\n",
    "    out_features: int\n",
    "    kernel_init: callable = nn.initializers.glorot_normal()\n",
    "    key = r.key()\n",
    "    mean = .5\n",
    "    std = .1\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Initialize s and w\n",
    "        s_init = self.mean + self.std * jax.random.normal(self.key, (self.out_features,))\n",
    "        s_init = jnp.exp(s_init)\n",
    "        w_init = self.kernel_init(self.key, (x.shape[-1], self.out_features))\n",
    "\n",
    "        # Calculate initial v\n",
    "        v_init = w_init / s_init\n",
    "\n",
    "        # Create trainable parameters\n",
    "        s = self.param('s', lambda rng: s_init)\n",
    "        v = self.param('v', lambda rng: v_init)\n",
    "        bias = self.param('bias', nn.initializers.zeros, (self.out_features,))\n",
    "\n",
    "        # Compute kernel\n",
    "        kernel = s * v  # shape will be (in_features, out_features)\n",
    "\n",
    "        # Apply the dot product: (batch_size, in_features) . (in_features, out_features) -> (batch_size, out_features)\n",
    "#        return jnp.matmul(x, kernel) + bias\n",
    "        return jnp.matmul(x, kernel) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24c57e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the MLP_fourier model initialization\n",
    "glorot = nn.initializers.glorot_normal()\n",
    "class RWF_fourier(nn.Module):\n",
    "    kernel_init: Callable\n",
    "    num_input: int\n",
    "    num_hidden: int\n",
    "    num_output: int\n",
    "    B: jnp.array\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Fourier feature embeddings\n",
    "        x = jnp.matmul(x,self.B.T)  # Apply the Fourier feature matrix\n",
    "        x = jnp.concatenate([jnp.cos(x), jnp.sin(x)], axis=-1)\n",
    "\n",
    "        # Hidden layers with Tanh activations\n",
    "        x = RWF(out_features=self.num_hidden, kernel_init=self.kernel_init)(x)\n",
    "        x = nn.tanh(x)\n",
    "        x = RWF(out_features=self.num_hidden, kernel_init=self.kernel_init)(x)\n",
    "        x = nn.tanh(x)\n",
    "        # Final output layer\n",
    "        x = RWF(out_features=self.num_output, kernel_init=self.kernel_init)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a6a0b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the MLP_fourier model initialization\n",
    "glorot = nn.initializers.glorot_normal()\n",
    "class MLP_fourier(nn.Module):\n",
    "    kernel_init: Callable\n",
    "    num_input: int\n",
    "    num_hidden: int\n",
    "    num_output: int\n",
    "    B: jnp.array\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Fourier feature embeddings\n",
    "        x = jnp.matmul(x,self.B.T)  # Apply the Fourier feature matrix\n",
    "        x = jnp.concatenate([jnp.cos(x), jnp.sin(x)], axis=-1)\n",
    "\n",
    "        # Hidden layers with Tanh activations\n",
    "        x = nn.Dense(self.num_hidden, kernel_init=self.kernel_init)(x)\n",
    "        x = nn.tanh(x)\n",
    "        x = nn.Dense(self.num_hidden, kernel_init=self.kernel_init)(x)\n",
    "        x = nn.tanh(x)\n",
    "\n",
    "        # Final output layer\n",
    "        x = nn.Dense(self.num_output, kernel_init=self.kernel_init)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c91aeed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_mse_loss(model, MODEL='MLP'):\n",
    "    @jax.jit\n",
    "    def mse_loss_mlp(params, x, y, state, loc_w):\n",
    "        def u(vec_x, variables):\n",
    "            y = model.apply(variables, vec_x)\n",
    "            return y\n",
    "        variables = {'params' : params}\n",
    "        \n",
    "        y_hat = u(x, variables)\n",
    "        loss = jnp.mean((y_hat - y)**2)\n",
    "\n",
    "        new_loc_w = loc_w\n",
    "        return loss, new_loc_w\n",
    "\n",
    "    if MODEL == 'MLP':\n",
    "        return mse_loss_mlp\n",
    "    \n",
    "    @jax.jit\n",
    "    def mse_loss_kan(params, x, y, state, loc_w):\n",
    "        def u(vec_x, variables):\n",
    "            y, _ = model.apply(variables, vec_x)\n",
    "            return y\n",
    "        variables = {'params' : params, 'state': state}\n",
    "        \n",
    "        y_hat = u(x, variables)\n",
    "        loss = jnp.mean((y_hat - y)**2)\n",
    "\n",
    "        new_loc_w = loc_w\n",
    "        return loss, new_loc_w\n",
    "    \n",
    "    if MODEL == 'KAN':\n",
    "        return mse_loss_kan\n",
    "    \n",
    "def get_train_step(model, optimizer, loss_fn):\n",
    "    @jax.jit\n",
    "    def train_step(params, x, y, opt_state, state, loc_w):\n",
    "        # Compute gradients and loss\n",
    "        grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "        (loss, new_loc_w), grads = grad_fn(params, x, y, state, loc_w)\n",
    "\n",
    "        # Update parameters using optimizer\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "\n",
    "        return params, opt_state, loss, new_loc_w\n",
    "\n",
    "    return train_step\n",
    "\n",
    "def sample_collocs():\n",
    "    # sample outside\n",
    "    collocs = jnp.array(sobol_sample(np.array([Config.X_MIN,Config.Y_MIN]), \n",
    "                                     np.array([Config.X_MAX,Config.Y_MAX]), Config.BS))\n",
    "    return collocs\n",
    "\n",
    "collocs = sample_collocs()\n",
    "collocs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5183be75",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add got incompatible shapes for broadcasting: (4096, 32), (4096, 128).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m RWF_fourier(glorot, num_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, B\u001b[38;5;241m=\u001b[39mB)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# model = MLP_fourier(glorot, num_input=2, num_hidden=32, num_output=1, B=B)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m variables \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m get_mse_loss(model, MODEL\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMLP\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m, in \u001b[0;36mRWF_fourier.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mconcatenate([jnp\u001b[38;5;241m.\u001b[39mcos(x), jnp\u001b[38;5;241m.\u001b[39msin(x)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Hidden layers with Tanh activations\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mRWF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_init\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mtanh(x)\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m RWF(out_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_hidden, kernel_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_init)(x)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m, in \u001b[0;36mRWF.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m         kernel \u001b[38;5;241m=\u001b[39m s \u001b[38;5;241m*\u001b[39m v  \u001b[38;5;66;03m# shape will be (in_features, out_features)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;66;03m# Apply the dot product: (batch_size, in_features) . (in_features, out_features) -> (batch_size, out_features)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#        return jnp.matmul(x, kernel) + bias\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/jax/_src/numpy/array_methods.py:265\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    263\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 265\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/jax/_src/numpy/ufuncs.py:102\u001b[0m, in \u001b[0;36m_maybe_bool_binop.<locals>.fn\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(x1, x2, \u001b[38;5;241m/\u001b[39m):\n\u001b[1;32m    101\u001b[0m   x1, x2 \u001b[38;5;241m=\u001b[39m promote_args(numpy_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, x1, x2)\n\u001b[0;32m--> 102\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m x1\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_ \u001b[38;5;28;01melse\u001b[39;00m bool_lax_fn(x1, x2)\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/jax/_src/lax/lax.py:1665\u001b[0m, in \u001b[0;36mbroadcasting_shape_rule\u001b[0;34m(name, *avals)\u001b[0m\n\u001b[1;32m   1663\u001b[0m       result_shape\u001b[38;5;241m.\u001b[39mappend(non_1s[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1664\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1665\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got incompatible shapes for broadcasting: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1666\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m, shapes)))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(result_shape)\n",
      "\u001b[0;31mTypeError\u001b[0m: add got incompatible shapes for broadcasting: (4096, 32), (4096, 128)."
     ]
    }
   ],
   "source": [
    "def u(vec_x, variables):\n",
    "    y = model.apply(variables, vec_x)\n",
    "    return y\n",
    "\n",
    "std = 5\n",
    "B = jax.random.normal(r.key(), (Config.HIDDEN_UNITS,2)) * std\n",
    "model = RWF_fourier(glorot, num_input=2, num_hidden=32, num_output=1, B=B)\n",
    "# model = MLP_fourier(glorot, num_input=2, num_hidden=32, num_output=1, B=B)\n",
    "\n",
    "variables = model.init(r.key(), collocs)\n",
    "loss_fn = get_mse_loss(model, MODEL='MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4820ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_and_plot_frequencies(input_array, N, high_freq_cutoff=0.25, title=\"Frequency Spectrum\"):\n",
    "    \"\"\"\n",
    "    Computes the 2D FFT of an input array, shifts the zero-frequency component to the center,\n",
    "    computes the magnitude spectrum, and plots it. Additionally, computes a metric summarizing the \n",
    "    importance of high frequencies based on energy concentration.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_array: A 1D or 2D JAX array of shape (N*N,) or (N, N).\n",
    "    - N: Integer, size of one dimension (input reshaped to (N, N)).\n",
    "    - high_freq_cutoff: A float (0 < high_freq_cutoff < 1), defines the fraction of frequencies to consider as high frequencies.\n",
    "    - title: String, title for the plot.\n",
    "\n",
    "    Returns:\n",
    "    - fft_shifted: The shifted 2D FFT result.\n",
    "    - high_freq_ratio: The computed ratio of high-frequency energy to total energy.\n",
    "    \"\"\"\n",
    "    # Reshape to 2D array if needed\n",
    "    if input_array.ndim == 1:\n",
    "        input_array = input_array.reshape(N, N)\n",
    "    \n",
    "    # Perform 2D FFT\n",
    "    fft_result = jnp.fft.fft2(input_array)\n",
    "    \n",
    "    # Shift zero-frequency component to the center\n",
    "    fft_shifted = jnp.fft.fftshift(fft_result)\n",
    "    \n",
    "    # Compute magnitude spectrum\n",
    "    magnitude = jnp.abs(fft_shifted)\n",
    "    \n",
    "    # Plotting the magnitude spectrum\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(jnp.log1p(magnitude), cmap='viridis', extent=(-N/2, N/2, -N/2, N/2))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Frequency X')\n",
    "    plt.ylabel('Frequency Y')\n",
    "    plt.colorbar(label='Log Magnitude')\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd82b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 300\n",
    "X_1 = jnp.linspace(Config.X_MIN, Config.X_MAX, N)\n",
    "X_2 = jnp.linspace(Config.Y_MIN, Config.Y_MAX, N)\n",
    "X_1, X_2 = jnp.meshgrid(X_1, X_2, indexing='ij')\n",
    "coords = jnp.stack([X_1.flatten(), X_2.flatten()], axis=1)\n",
    "\n",
    "y_hat = u(coords, variables).reshape(N,N)\n",
    "compute_and_plot_frequencies(y_hat, N, title=\"Initial Freq Spectrum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a74fa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "FREQ = 8\n",
    "# Function to test interpolation\n",
    "@jax.jit\n",
    "def oscillatory_sine(x):\n",
    "    y = jnp.sin(FREQ * jnp.pi * x[:, 0]) * jnp.cos(FREQ * jnp.pi * x[:, 1])\n",
    "    return y\n",
    "\n",
    "@jax.jit\n",
    "def radial_oscillation(x):\n",
    "    r = jnp.sqrt((x[:, 0] - 1)**2 + (x[:, 1] - 1)**2)\n",
    "    y = jnp.sin(FREQ * jnp.pi * r)\n",
    "    return y\n",
    "\n",
    "@jax.jit\n",
    "def discontinuous_starburst(x):\n",
    "    theta = jnp.arctan2(x[:, 1] - 1, x[:, 0] - 1)\n",
    "    y = jnp.where(jnp.mod(theta * FREQ, 2 * jnp.pi) > jnp.pi, 1.0, -1.0)\n",
    "    return y\n",
    "\n",
    "@jax.jit\n",
    "def ring_function(x):\n",
    "    y = jnp.sin(FREQ * jnp.pi * jnp.sqrt((x[:, 0] - 1)**2 + (x[:, 1] - 1)**2))\n",
    "    return y\n",
    "\n",
    "@jax.jit\n",
    "def modulated_gaussian(x):\n",
    "    y = jnp.exp(-((x[:, 0] - 1)**2 + (x[:, 1] - 1)**2)) * \\\n",
    "        jnp.sin(FREQ * jnp.pi * x[:, 0]) * jnp.cos(FREQ * jnp.pi * x[:, 1])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff8dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "def u(vec_x, variables):\n",
    "    y = model.apply(variables, vec_x)\n",
    "    return y\n",
    "\n",
    "loc_w = jnp.array([])\n",
    "\n",
    "# Define a cosine decay learning rate schedule\n",
    "schedule_fn = optax.cosine_decay_schedule(\n",
    "    init_value=1e-2,       # Initial learning rate\n",
    "    decay_steps=Config.EPOCHS,  # Total number of decay steps\n",
    "    alpha=1e-3             # Final learning rate multiplier\n",
    ")\n",
    "\n",
    "optimizer = optax.adamw(learning_rate=schedule_fn, weight_decay=1e-4)\n",
    "opt_state = optimizer.init(variables['params'])\n",
    "train_step = get_train_step(model, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ced4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "learnable_func = modulated_gaussian\n",
    "\n",
    "grid_adaption = {}\n",
    "\n",
    "if len(variables) == 1:\n",
    "    variables[\"state\"] = []\n",
    "    \n",
    "# train always on same colloc points\n",
    "collocs = sample_collocs()\n",
    "losses = []\n",
    "\n",
    "for i in (pbar:= tqdm(range(Config.EPOCHS))):\n",
    "    params, state = variables['params'], variables['state']\n",
    "    y = learnable_func(collocs).reshape(-1,1)\n",
    "    params, opt_state, loss, loc_w = train_step(params, collocs, y,\n",
    "                                                opt_state, state, loc_w)\n",
    "    variables = {'params': params, 'state':state}\n",
    "        \n",
    "    losses.append(loss)\n",
    "    \n",
    "    if i % 50 == 0: # dont waste a lot of time printing\n",
    "        pbar.set_description(f\"Loss {loss: .8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147473ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(arr, title):\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(arr, extent=(Config.X_MIN, Config.X_MAX, Config.Y_MIN, Config.Y_MAX), \n",
    "               origin=\"lower\", aspect=\"auto\", cmap=\"plasma\")\n",
    "    plt.colorbar(label=\"Loss\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "N = 300\n",
    "\n",
    "X_1 = jnp.linspace(Config.X_MIN, Config.X_MAX, N)\n",
    "X_2 = jnp.linspace(Config.Y_MIN, Config.Y_MAX, N)\n",
    "X_1, X_2 = jnp.meshgrid(X_1, X_2, indexing='ij')\n",
    "coords = jnp.stack([X_1.flatten(), X_2.flatten()], axis=1)\n",
    "y = learnable_func(coords).reshape(-1,1)\n",
    "\n",
    "if Config.MODEL == 'MLP':\n",
    "    y_hat = u(coords, {'params': variables[\"params\"]})\n",
    "else:\n",
    "    y_hat = u(coords, variables)\n",
    "error = jnp.abs(y - y_hat).reshape(N, N)\n",
    "\n",
    "plot_heatmap(error, \"Abs error\")\n",
    "# plot_heatmap(error / (y.mean() + 1e-16), \"Relative error\")\n",
    "\n",
    "\"\"\"\n",
    "grid_extensions = {200: 5, 400: 10}\n",
    "for index in grid_extensions.keys():\n",
    "    plt.axvline(x=index, color='red', linestyle='--')\n",
    "\"\"\"\n",
    "# plt.plot(losses)\n",
    "plt.semilogy(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305ac1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "def compute_l2_error(results, true):\n",
    "    error = jnp.linalg.norm(results - true) / jnp.linalg.norm(true)\n",
    "    return error\n",
    "    \n",
    "def plot_3d_surfaces_with_2d(X_1, X_2, p1, p2, error=None, colloc_points=None, \n",
    "                             title1='Interpolated', title2='Ground Truth', title3='Error', figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Plots two 3D surface plots with their 2D projections and overlays collocation points.\n",
    "\n",
    "    Parameters:\n",
    "    - X_1: 2D array for X-axis values.\n",
    "    - X_2: 2D array for Y-axis values.\n",
    "    - p1: 2D array for Z-axis values of the first surface (e.g., interpolated values).\n",
    "    - p2: 2D array for Z-axis values of the second surface (e.g., ground truth values).\n",
    "    - error: Optional 2D array for Z-axis values of the error surface.\n",
    "    - colloc_points: Optional tuple (X, Y, Z) of collocation points to overlay.\n",
    "    - title1: Title for the first surface plot.\n",
    "    - title2: Title for the second surface plot.\n",
    "    - title3: Title for the error surface plot (if provided).\n",
    "    - figsize: Size of the figure (width, height).\n",
    "    \"\"\"\n",
    "    # Create a larger figure with multiple subplots\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    # 3D plot for p1\n",
    "    ax1 = fig.add_subplot(231, projection='3d')\n",
    "    surf1 = ax1.plot_surface(X_1, X_2, p1, cmap=cm.plasma, linewidth=0, antialiased=False)\n",
    "    ax1.set_title(title1)\n",
    "    fig.colorbar(surf1, ax=ax1, shrink=0.5, aspect=10)\n",
    "\n",
    "\n",
    "    # 3D plot for p2\n",
    "    ax2 = fig.add_subplot(232, projection='3d')\n",
    "    surf2 = ax2.plot_surface(X_1, X_2, p2, cmap=cm.plasma, linewidth=0, antialiased=False)\n",
    "    ax2.set_title(title2)\n",
    "    fig.colorbar(surf2, ax=ax2, shrink=0.5, aspect=10)\n",
    "\n",
    "    # Error plot if provided\n",
    "    if error is not None:\n",
    "        ax3 = fig.add_subplot(233, projection='3d')\n",
    "        surf3 = ax3.plot_surface(X_1, X_2, error, cmap=cm.inferno, linewidth=0, antialiased=False)\n",
    "        ax3.set_title(title3)\n",
    "        fig.colorbar(surf3, ax=ax3, shrink=0.5, aspect=10)\n",
    "\n",
    "    # Add 2D projections below each 3D plot\n",
    "    ax4 = fig.add_subplot(234)\n",
    "    im1 = ax4.imshow(p1, extent=(X_1.min(), X_1.max(), X_2.min(), X_2.max()), origin='lower', cmap=cm.plasma)\n",
    "    ax4.set_title(f\"2D {title1}\")\n",
    "    fig.colorbar(im1, ax=ax4)\n",
    "\n",
    "    ax5 = fig.add_subplot(235)\n",
    "    im2 = ax5.imshow(p2, extent=(X_1.min(), X_1.max(), X_2.min(), X_2.max()), origin='lower', cmap=cm.plasma)\n",
    "    ax5.set_title(f\"2D {title2}\")\n",
    "    fig.colorbar(im2, ax=ax5)\n",
    "\n",
    "    if error is not None:\n",
    "        ax6 = fig.add_subplot(236)\n",
    "        im3 = ax6.imshow(error, extent=(X_1.min(), X_1.max(), X_2.min(), X_2.max()), origin='lower', cmap=cm.inferno)\n",
    "        ax6.set_title(f\"2D {title3}\")\n",
    "        fig.colorbar(im3, ax=ax6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "y = y.reshape(N, N)\n",
    "y_hat = y_hat.reshape(N, N)\n",
    "\n",
    "print(f\"MODEL USED: \\t\\t{Config.MODEL}\")\n",
    "print(f\"FOURIER FEATURES: \\t{Config.FourierFeatures}\")\n",
    "print(f'L2 ERROR \\t\\t{compute_l2_error(y_hat, y):.4f}%')\n",
    "# L2 ERROR \t\t5.3785% RWF\n",
    "# L2 ERROR \t\t6.1691% MLP\n",
    "\n",
    "plot_3d_surfaces_with_2d(X_1, X_2, y_hat, y, error, collocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Assuming y_hat and y are given and reshaped into (N, N)\n",
    "# Perform 2D FFT\n",
    "fft_y_hat = jnp.fft.fft2(y_hat.reshape(N, N))\n",
    "fft_y = jnp.fft.fft2(y.reshape(N, N))\n",
    "\n",
    "# Shift zero-frequency component to the center\n",
    "fft_y_hat_shifted = jnp.fft.fftshift(fft_y_hat)\n",
    "fft_y_shifted = jnp.fft.fftshift(fft_y)\n",
    "\n",
    "# Compute magnitude spectra\n",
    "magnitude_y_hat = jnp.abs(fft_y_hat_shifted)\n",
    "magnitude_y = jnp.abs(fft_y_shifted)\n",
    "\n",
    "# Plotting the magnitude spectra\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].imshow(jnp.log1p(magnitude_y_hat), cmap='viridis', extent=(-N/2, N/2, -N/2, N/2))\n",
    "axes[0].set_title('Frequency Spectrum of y_hat')\n",
    "axes[0].set_xlabel('Frequency X')\n",
    "axes[0].set_ylabel('Frequency Y')\n",
    "\n",
    "axes[1].imshow(jnp.log1p(magnitude_y), cmap='viridis', extent=(-N/2, N/2, -N/2, N/2))\n",
    "axes[1].set_title('Frequency Spectrum of y')\n",
    "axes[1].set_xlabel('Frequency X')\n",
    "axes[1].set_ylabel('Frequency Y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cded24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
